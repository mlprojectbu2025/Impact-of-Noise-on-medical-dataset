{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Heart Disease Label Noise Analysis (without SMOTE)\n",
        "\"\"\"\n",
        "\n",
        "!pip install ucimlrepo\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "###############################################################################\n",
        "# 1) Load and Preprocess the Heart Disease Data\n",
        "###############################################################################\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "print(\"Fetching Heart Disease dataset from UCIML...\")\n",
        "heart_disease = fetch_ucirepo(id=45)\n",
        "X = heart_disease.data.features\n",
        "y = heart_disease.data.targets\n",
        "\n",
        "# Combine features and target into a single DataFrame\n",
        "data = pd.concat([X, y], axis=1)\n",
        "\n",
        "# Convert all columns to numeric\n",
        "for col in data.columns:\n",
        "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
        "\n",
        "# Drop rows with missing values\n",
        "data = data.dropna()\n",
        "\n",
        "print(\"First 5 rows of the dataset after numeric conversion and dropping NAs:\")\n",
        "print(data.head())\n",
        "\n",
        "print(\"\\nDataset Info:\")\n",
        "print(data.info())\n",
        "\n",
        "# Convert target column 'num' to binary: 1 if >0 else 0\n",
        "data['num'] = data['num'].apply(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "target = 'num'\n",
        "features = data.columns.drop(target)\n",
        "\n",
        "# One-hot encode categorical variables (if present)\n",
        "categorical_vars = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal', 'ca']\n",
        "data_encoded = pd.get_dummies(data, columns=[c for c in categorical_vars if c in data.columns], drop_first=True)\n",
        "\n",
        "X = data_encoded.drop(columns=[target], errors='ignore')\n",
        "y = data_encoded[target].astype(int)\n",
        "\n",
        "print(\"\\nClass Distribution in Dataset (0 = no disease, 1 = disease):\")\n",
        "print(y.value_counts())\n",
        "\n",
        "###############################################################################\n",
        "# 2) Define Models\n",
        "###############################################################################\n",
        "\n",
        "model_colors = {\n",
        "    'Logistic Regression': 'blue',\n",
        "    'Decision Tree': 'green',\n",
        "    'Naive Bayes': 'purple',\n",
        "    'K-Nearest Neighbors': 'brown',\n",
        "    'Support Vector Machine': 'red',\n",
        "    'Random Forest': 'orange'\n",
        "}\n",
        "\n",
        "models = {\n",
        "    'Support Vector Machine': SVC(probability=True, random_state=RANDOM_STATE),\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=RANDOM_STATE),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
        "}\n",
        "\n",
        "###############################################################################\n",
        "# 3) Simulate Label Noise, Train, and Collect Metrics\n",
        "###############################################################################\n",
        "\n",
        "noise_levels = np.arange(0.00, 0.45, 0.05)\n",
        "\n",
        "# Data structures to store metrics and aggregated confusion matrices\n",
        "ACC_values = {m: {nl: [] for nl in noise_levels} for m in models}\n",
        "TPR_values = {m: {nl: [] for nl in noise_levels} for m in models}\n",
        "TNR_values = {m: {nl: [] for nl in noise_levels} for m in models}\n",
        "confusion_matrix_sums = {m: {nl: np.zeros((2, 2), dtype=int) for nl in noise_levels} for m in models}\n",
        "\n",
        "print(\"\\nStarting model training and evaluation with simulated label noise (without SMOTE)...\\n\")\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"Evaluating {model_name}...\")\n",
        "    for noise_level in noise_levels:\n",
        "        conf_matrix_sum = np.zeros((2, 2), dtype=int)\n",
        "        for iteration in range(10):\n",
        "            iter_seed = RANDOM_STATE + iteration\n",
        "\n",
        "            # Split the data for this iteration\n",
        "            X_train_, X_test_, y_train_, y_test_ = train_test_split(\n",
        "                X, y, test_size=0.5, stratify=y, random_state=iter_seed\n",
        "            )\n",
        "\n",
        "            # Scale features\n",
        "            scaler_iter = StandardScaler()\n",
        "            X_train_scaled_ = scaler_iter.fit_transform(X_train_)\n",
        "            X_test_scaled_ = scaler_iter.transform(X_test_)\n",
        "\n",
        "            # Introduce label noise in the training set\n",
        "            y_train_noisy = y_train_.copy()\n",
        "            num_noisy = int(noise_level * len(y_train_noisy))\n",
        "            np.random.seed(iter_seed)\n",
        "            noisy_indices = np.random.choice(len(y_train_noisy), size=num_noisy, replace=False)\n",
        "            y_train_noisy.iloc[noisy_indices] = 1 - y_train_noisy.iloc[noisy_indices]\n",
        "\n",
        "            # Train the model without applying SMOTE\n",
        "            model.fit(X_train_scaled_, y_train_noisy)\n",
        "            y_pred = model.predict(X_test_scaled_)\n",
        "\n",
        "            cm = confusion_matrix(y_test_, y_pred, labels=[0, 1])\n",
        "            conf_matrix_sum += cm\n",
        "\n",
        "            TN, FP, FN, TP = cm.ravel()\n",
        "            acc = accuracy_score(y_test_, y_pred)\n",
        "            tpr = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
        "            tnr = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
        "\n",
        "            ACC_values[model_name][noise_level].append(acc)\n",
        "            TPR_values[model_name][noise_level].append(tpr)\n",
        "            TNR_values[model_name][noise_level].append(tnr)\n",
        "\n",
        "        confusion_matrix_sums[model_name][noise_level] = conf_matrix_sum\n",
        "        print(f\"  Noise Level: {int(noise_level * 100)}%\")\n",
        "        print(\"  Aggregated Confusion Matrix:\\n\", conf_matrix_sum)\n",
        "    print(f\"-> Completed evaluations for {model_name}\\n\")\n",
        "\n",
        "print(\"All models evaluated under different noise levels.\")\n",
        "\n",
        "###############################################################################\n",
        "# 4) Compute Metrics from Aggregated Confusion Matrices and Print Markdown Tables\n",
        "###############################################################################\n",
        "\n",
        "print(\"\\n### Final Results for Heart Disease Dataset\\n\")\n",
        "for model_name in models.keys():\n",
        "    print(f\"#### Model: {model_name}\\n\")\n",
        "    print(\"| Noise |  TN  |  FP  |  FN  |  TP  | Accuracy |  TPR  |  TNR  |  PPV  | F1 Score |\")\n",
        "    print(\"|:-----:|:----:|:----:|:----:|:----:|:--------:|:-----:|:-----:|:-----:|:--------:|\")\n",
        "    for nl in noise_levels:\n",
        "        cm = confusion_matrix_sums[model_name][nl]\n",
        "        TN, FP, FN, TP = cm.ravel()\n",
        "        total = TN + FP + FN + TP\n",
        "        accuracy = (TN + TP) / total if total != 0 else 0\n",
        "        tpr = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "        tnr = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
        "        ppv = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "        f1 = (2 * tpr * ppv / (tpr + ppv)) if (tpr + ppv) > 0 else 0\n",
        "        print(f\"| {int(nl*100)}% | {TN} | {FP} | {FN} | {TP} | {accuracy:.3f} | {tpr:.3f} | {tnr:.3f} | {ppv:.3f} | {f1:.3f} |\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pr5AI_IzRPiW",
        "outputId": "21315e67-e228-4e78-f300-218528730275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ucimlrepo\n",
            "  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo) (2025.1.31)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n",
            "Downloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\n",
            "Installing collected packages: ucimlrepo\n",
            "Successfully installed ucimlrepo-0.0.7\n",
            "Fetching Heart Disease dataset from UCIML...\n",
            "First 5 rows of the dataset after numeric conversion and dropping NAs:\n",
            "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
            "0   63    1   1       145   233    1        2      150      0      2.3      3   \n",
            "1   67    1   4       160   286    0        2      108      1      1.5      2   \n",
            "2   67    1   4       120   229    0        2      129      1      2.6      2   \n",
            "3   37    1   3       130   250    0        0      187      0      3.5      3   \n",
            "4   41    0   2       130   204    0        2      172      0      1.4      1   \n",
            "\n",
            "    ca  thal  num  \n",
            "0  0.0   6.0    0  \n",
            "1  3.0   3.0    2  \n",
            "2  2.0   7.0    1  \n",
            "3  0.0   3.0    0  \n",
            "4  0.0   3.0    0  \n",
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 297 entries, 0 to 301\n",
            "Data columns (total 14 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   age       297 non-null    int64  \n",
            " 1   sex       297 non-null    int64  \n",
            " 2   cp        297 non-null    int64  \n",
            " 3   trestbps  297 non-null    int64  \n",
            " 4   chol      297 non-null    int64  \n",
            " 5   fbs       297 non-null    int64  \n",
            " 6   restecg   297 non-null    int64  \n",
            " 7   thalach   297 non-null    int64  \n",
            " 8   exang     297 non-null    int64  \n",
            " 9   oldpeak   297 non-null    float64\n",
            " 10  slope     297 non-null    int64  \n",
            " 11  ca        297 non-null    float64\n",
            " 12  thal      297 non-null    float64\n",
            " 13  num       297 non-null    int64  \n",
            "dtypes: float64(3), int64(11)\n",
            "memory usage: 34.8 KB\n",
            "None\n",
            "\n",
            "Class Distribution in Dataset (0 = no disease, 1 = disease):\n",
            "num\n",
            "0    160\n",
            "1    137\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Starting model training and evaluation with simulated label noise (without SMOTE)...\n",
            "\n",
            "Evaluating Support Vector Machine...\n",
            "  Noise Level: 0%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[685 115]\n",
            " [163 527]]\n",
            "  Noise Level: 5%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[681 119]\n",
            " [172 518]]\n",
            "  Noise Level: 10%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[673 127]\n",
            " [182 508]]\n",
            "  Noise Level: 15%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[673 127]\n",
            " [190 500]]\n",
            "  Noise Level: 20%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[659 141]\n",
            " [186 504]]\n",
            "  Noise Level: 25%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[642 158]\n",
            " [193 497]]\n",
            "  Noise Level: 30%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[627 173]\n",
            " [206 484]]\n",
            "  Noise Level: 35%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[596 204]\n",
            " [229 461]]\n",
            "  Noise Level: 40%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[532 268]\n",
            " [235 455]]\n",
            "-> Completed evaluations for Support Vector Machine\n",
            "\n",
            "Evaluating Logistic Regression...\n",
            "  Noise Level: 0%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[685 115]\n",
            " [154 536]]\n",
            "  Noise Level: 5%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[672 128]\n",
            " [171 519]]\n",
            "  Noise Level: 10%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[658 142]\n",
            " [170 520]]\n",
            "  Noise Level: 15%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[642 158]\n",
            " [189 501]]\n",
            "  Noise Level: 20%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[640 160]\n",
            " [192 498]]\n",
            "  Noise Level: 25%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[598 202]\n",
            " [202 488]]\n",
            "  Noise Level: 30%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[578 222]\n",
            " [214 476]]\n",
            "  Noise Level: 35%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[543 257]\n",
            " [226 464]]\n",
            "  Noise Level: 40%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[494 306]\n",
            " [240 450]]\n",
            "-> Completed evaluations for Logistic Regression\n",
            "\n",
            "Evaluating K-Nearest Neighbors...\n",
            "  Noise Level: 0%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[695 105]\n",
            " [201 489]]\n",
            "  Noise Level: 5%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[686 114]\n",
            " [202 488]]\n",
            "  Noise Level: 10%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[672 128]\n",
            " [192 498]]\n",
            "  Noise Level: 15%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[656 144]\n",
            " [212 478]]\n",
            "  Noise Level: 20%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[647 153]\n",
            " [225 465]]\n",
            "  Noise Level: 25%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[594 206]\n",
            " [230 460]]\n",
            "  Noise Level: 30%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[567 233]\n",
            " [235 455]]\n",
            "  Noise Level: 35%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[535 265]\n",
            " [252 438]]\n",
            "  Noise Level: 40%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[494 306]\n",
            " [272 418]]\n",
            "-> Completed evaluations for K-Nearest Neighbors\n",
            "\n",
            "Evaluating Decision Tree...\n",
            "  Noise Level: 0%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[592 208]\n",
            " [220 470]]\n",
            "  Noise Level: 5%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[564 236]\n",
            " [224 466]]\n",
            "  Noise Level: 10%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[546 254]\n",
            " [218 472]]\n",
            "  Noise Level: 15%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[513 287]\n",
            " [255 435]]\n",
            "  Noise Level: 20%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[529 271]\n",
            " [281 409]]\n",
            "  Noise Level: 25%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[507 293]\n",
            " [284 406]]\n",
            "  Noise Level: 30%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[461 339]\n",
            " [281 409]]\n",
            "  Noise Level: 35%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[455 345]\n",
            " [297 393]]\n",
            "  Noise Level: 40%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[442 358]\n",
            " [330 360]]\n",
            "-> Completed evaluations for Decision Tree\n",
            "\n",
            "Evaluating Naive Bayes...\n",
            "  Noise Level: 0%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[633 167]\n",
            " [281 409]]\n",
            "  Noise Level: 5%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[631 169]\n",
            " [322 368]]\n",
            "  Noise Level: 10%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[483 317]\n",
            " [276 414]]\n",
            "  Noise Level: 15%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[467 333]\n",
            " [287 403]]\n",
            "  Noise Level: 20%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[448 352]\n",
            " [254 436]]\n",
            "  Noise Level: 25%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[439 361]\n",
            " [261 429]]\n",
            "  Noise Level: 30%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[440 360]\n",
            " [270 420]]\n",
            "  Noise Level: 35%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[410 390]\n",
            " [227 463]]\n",
            "  Noise Level: 40%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[383 417]\n",
            " [237 453]]\n",
            "-> Completed evaluations for Naive Bayes\n",
            "\n",
            "Evaluating Random Forest...\n",
            "  Noise Level: 0%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[680 120]\n",
            " [194 496]]\n",
            "  Noise Level: 5%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[678 122]\n",
            " [186 504]]\n",
            "  Noise Level: 10%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[656 144]\n",
            " [176 514]]\n",
            "  Noise Level: 15%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[661 139]\n",
            " [200 490]]\n",
            "  Noise Level: 20%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[635 165]\n",
            " [193 497]]\n",
            "  Noise Level: 25%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[597 203]\n",
            " [209 481]]\n",
            "  Noise Level: 30%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[566 234]\n",
            " [228 462]]\n",
            "  Noise Level: 35%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[530 270]\n",
            " [245 445]]\n",
            "  Noise Level: 40%\n",
            "  Aggregated Confusion Matrix:\n",
            " [[498 302]\n",
            " [265 425]]\n",
            "-> Completed evaluations for Random Forest\n",
            "\n",
            "All models evaluated under different noise levels.\n",
            "\n",
            "### Final Results for Heart Disease Dataset\n",
            "\n",
            "#### Model: Support Vector Machine\n",
            "\n",
            "| Noise |  TN  |  FP  |  FN  |  TP  | Accuracy |  TPR  |  TNR  |  PPV  | F1 Score |\n",
            "|:-----:|:----:|:----:|:----:|:----:|:--------:|:-----:|:-----:|:-----:|:--------:|\n",
            "| 0% | 685 | 115 | 163 | 527 | 0.813 | 0.764 | 0.856 | 0.821 | 0.791 |\n",
            "| 5% | 681 | 119 | 172 | 518 | 0.805 | 0.751 | 0.851 | 0.813 | 0.781 |\n",
            "| 10% | 673 | 127 | 182 | 508 | 0.793 | 0.736 | 0.841 | 0.800 | 0.767 |\n",
            "| 15% | 673 | 127 | 190 | 500 | 0.787 | 0.725 | 0.841 | 0.797 | 0.759 |\n",
            "| 20% | 659 | 141 | 186 | 504 | 0.781 | 0.730 | 0.824 | 0.781 | 0.755 |\n",
            "| 25% | 642 | 158 | 193 | 497 | 0.764 | 0.720 | 0.802 | 0.759 | 0.739 |\n",
            "| 30% | 627 | 173 | 206 | 484 | 0.746 | 0.701 | 0.784 | 0.737 | 0.719 |\n",
            "| 35% | 596 | 204 | 229 | 461 | 0.709 | 0.668 | 0.745 | 0.693 | 0.680 |\n",
            "| 40% | 532 | 268 | 235 | 455 | 0.662 | 0.659 | 0.665 | 0.629 | 0.644 |\n",
            "\n",
            "\n",
            "#### Model: Logistic Regression\n",
            "\n",
            "| Noise |  TN  |  FP  |  FN  |  TP  | Accuracy |  TPR  |  TNR  |  PPV  | F1 Score |\n",
            "|:-----:|:----:|:----:|:----:|:----:|:--------:|:-----:|:-----:|:-----:|:--------:|\n",
            "| 0% | 685 | 115 | 154 | 536 | 0.819 | 0.777 | 0.856 | 0.823 | 0.799 |\n",
            "| 5% | 672 | 128 | 171 | 519 | 0.799 | 0.752 | 0.840 | 0.802 | 0.776 |\n",
            "| 10% | 658 | 142 | 170 | 520 | 0.791 | 0.754 | 0.823 | 0.785 | 0.769 |\n",
            "| 15% | 642 | 158 | 189 | 501 | 0.767 | 0.726 | 0.802 | 0.760 | 0.743 |\n",
            "| 20% | 640 | 160 | 192 | 498 | 0.764 | 0.722 | 0.800 | 0.757 | 0.739 |\n",
            "| 25% | 598 | 202 | 202 | 488 | 0.729 | 0.707 | 0.748 | 0.707 | 0.707 |\n",
            "| 30% | 578 | 222 | 214 | 476 | 0.707 | 0.690 | 0.723 | 0.682 | 0.686 |\n",
            "| 35% | 543 | 257 | 226 | 464 | 0.676 | 0.672 | 0.679 | 0.644 | 0.658 |\n",
            "| 40% | 494 | 306 | 240 | 450 | 0.634 | 0.652 | 0.618 | 0.595 | 0.622 |\n",
            "\n",
            "\n",
            "#### Model: K-Nearest Neighbors\n",
            "\n",
            "| Noise |  TN  |  FP  |  FN  |  TP  | Accuracy |  TPR  |  TNR  |  PPV  | F1 Score |\n",
            "|:-----:|:----:|:----:|:----:|:----:|:--------:|:-----:|:-----:|:-----:|:--------:|\n",
            "| 0% | 695 | 105 | 201 | 489 | 0.795 | 0.709 | 0.869 | 0.823 | 0.762 |\n",
            "| 5% | 686 | 114 | 202 | 488 | 0.788 | 0.707 | 0.858 | 0.811 | 0.755 |\n",
            "| 10% | 672 | 128 | 192 | 498 | 0.785 | 0.722 | 0.840 | 0.796 | 0.757 |\n",
            "| 15% | 656 | 144 | 212 | 478 | 0.761 | 0.693 | 0.820 | 0.768 | 0.729 |\n",
            "| 20% | 647 | 153 | 225 | 465 | 0.746 | 0.674 | 0.809 | 0.752 | 0.711 |\n",
            "| 25% | 594 | 206 | 230 | 460 | 0.707 | 0.667 | 0.743 | 0.691 | 0.678 |\n",
            "| 30% | 567 | 233 | 235 | 455 | 0.686 | 0.659 | 0.709 | 0.661 | 0.660 |\n",
            "| 35% | 535 | 265 | 252 | 438 | 0.653 | 0.635 | 0.669 | 0.623 | 0.629 |\n",
            "| 40% | 494 | 306 | 272 | 418 | 0.612 | 0.606 | 0.618 | 0.577 | 0.591 |\n",
            "\n",
            "\n",
            "#### Model: Decision Tree\n",
            "\n",
            "| Noise |  TN  |  FP  |  FN  |  TP  | Accuracy |  TPR  |  TNR  |  PPV  | F1 Score |\n",
            "|:-----:|:----:|:----:|:----:|:----:|:--------:|:-----:|:-----:|:-----:|:--------:|\n",
            "| 0% | 592 | 208 | 220 | 470 | 0.713 | 0.681 | 0.740 | 0.693 | 0.687 |\n",
            "| 5% | 564 | 236 | 224 | 466 | 0.691 | 0.675 | 0.705 | 0.664 | 0.670 |\n",
            "| 10% | 546 | 254 | 218 | 472 | 0.683 | 0.684 | 0.682 | 0.650 | 0.667 |\n",
            "| 15% | 513 | 287 | 255 | 435 | 0.636 | 0.630 | 0.641 | 0.602 | 0.616 |\n",
            "| 20% | 529 | 271 | 281 | 409 | 0.630 | 0.593 | 0.661 | 0.601 | 0.597 |\n",
            "| 25% | 507 | 293 | 284 | 406 | 0.613 | 0.588 | 0.634 | 0.581 | 0.585 |\n",
            "| 30% | 461 | 339 | 281 | 409 | 0.584 | 0.593 | 0.576 | 0.547 | 0.569 |\n",
            "| 35% | 455 | 345 | 297 | 393 | 0.569 | 0.570 | 0.569 | 0.533 | 0.550 |\n",
            "| 40% | 442 | 358 | 330 | 360 | 0.538 | 0.522 | 0.552 | 0.501 | 0.511 |\n",
            "\n",
            "\n",
            "#### Model: Naive Bayes\n",
            "\n",
            "| Noise |  TN  |  FP  |  FN  |  TP  | Accuracy |  TPR  |  TNR  |  PPV  | F1 Score |\n",
            "|:-----:|:----:|:----:|:----:|:----:|:--------:|:-----:|:-----:|:-----:|:--------:|\n",
            "| 0% | 633 | 167 | 281 | 409 | 0.699 | 0.593 | 0.791 | 0.710 | 0.646 |\n",
            "| 5% | 631 | 169 | 322 | 368 | 0.670 | 0.533 | 0.789 | 0.685 | 0.600 |\n",
            "| 10% | 483 | 317 | 276 | 414 | 0.602 | 0.600 | 0.604 | 0.566 | 0.583 |\n",
            "| 15% | 467 | 333 | 287 | 403 | 0.584 | 0.584 | 0.584 | 0.548 | 0.565 |\n",
            "| 20% | 448 | 352 | 254 | 436 | 0.593 | 0.632 | 0.560 | 0.553 | 0.590 |\n",
            "| 25% | 439 | 361 | 261 | 429 | 0.583 | 0.622 | 0.549 | 0.543 | 0.580 |\n",
            "| 30% | 440 | 360 | 270 | 420 | 0.577 | 0.609 | 0.550 | 0.538 | 0.571 |\n",
            "| 35% | 410 | 390 | 227 | 463 | 0.586 | 0.671 | 0.512 | 0.543 | 0.600 |\n",
            "| 40% | 383 | 417 | 237 | 453 | 0.561 | 0.657 | 0.479 | 0.521 | 0.581 |\n",
            "\n",
            "\n",
            "#### Model: Random Forest\n",
            "\n",
            "| Noise |  TN  |  FP  |  FN  |  TP  | Accuracy |  TPR  |  TNR  |  PPV  | F1 Score |\n",
            "|:-----:|:----:|:----:|:----:|:----:|:--------:|:-----:|:-----:|:-----:|:--------:|\n",
            "| 0% | 680 | 120 | 194 | 496 | 0.789 | 0.719 | 0.850 | 0.805 | 0.760 |\n",
            "| 5% | 678 | 122 | 186 | 504 | 0.793 | 0.730 | 0.848 | 0.805 | 0.766 |\n",
            "| 10% | 656 | 144 | 176 | 514 | 0.785 | 0.745 | 0.820 | 0.781 | 0.763 |\n",
            "| 15% | 661 | 139 | 200 | 490 | 0.772 | 0.710 | 0.826 | 0.779 | 0.743 |\n",
            "| 20% | 635 | 165 | 193 | 497 | 0.760 | 0.720 | 0.794 | 0.751 | 0.735 |\n",
            "| 25% | 597 | 203 | 209 | 481 | 0.723 | 0.697 | 0.746 | 0.703 | 0.700 |\n",
            "| 30% | 566 | 234 | 228 | 462 | 0.690 | 0.670 | 0.708 | 0.664 | 0.667 |\n",
            "| 35% | 530 | 270 | 245 | 445 | 0.654 | 0.645 | 0.662 | 0.622 | 0.633 |\n",
            "| 40% | 498 | 302 | 265 | 425 | 0.619 | 0.616 | 0.623 | 0.585 | 0.600 |\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}