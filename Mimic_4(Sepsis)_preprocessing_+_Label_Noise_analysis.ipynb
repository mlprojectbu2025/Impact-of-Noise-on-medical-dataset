{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTNGX6LE_-6C"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import dask.dataframe as dd\n",
        "from dask.distributed import Client\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    data_dir = '/Users/angam/mimic_project'  # Adjust if needed\n",
        "\n",
        "    # Initialize Dask client\n",
        "    client = Client(n_workers=6)\n",
        "    print(\"Dask client initialized with 6 workers.\")\n",
        "\n",
        "    # Name of the file to save processed data\n",
        "    processed_data_path = 'processed_data.pkl'\n",
        "\n",
        "    # ---------------------------\n",
        "    # Step 1: Load Core Tables and Define Target (Sepsis)\n",
        "    # ---------------------------\n",
        "    print(\"Loading core tables...\")\n",
        "    patients = pd.read_csv(os.path.join(data_dir, 'patients.csv'), usecols=['subject_id', 'gender', 'anchor_age', 'anchor_year_group'])\n",
        "    admissions = pd.read_csv(os.path.join(data_dir, 'admissions.csv'), usecols=['subject_id', 'hadm_id', 'admittime', 'dischtime', 'hospital_expire_flag', 'insurance', 'marital_status'])\n",
        "    icustays = pd.read_csv(os.path.join(data_dir, 'icustays.csv'), usecols=['subject_id', 'hadm_id', 'stay_id', 'intime', 'outtime'])\n",
        "\n",
        "    data = admissions.merge(patients, on='subject_id', how='inner')\n",
        "    data = data.merge(icustays, on=['subject_id', 'hadm_id'], how='inner')\n",
        "    print(\"Merged data shape:\", data.shape)\n",
        "\n",
        "    # Load diagnoses for sepsis labeling\n",
        "    diagnoses_icd = pd.read_csv(os.path.join(data_dir, 'diagnoses_icd.csv'), usecols=['subject_id', 'hadm_id', 'icd_code', 'icd_version'])\n",
        "    d_icd_diagnoses = pd.read_csv(os.path.join(data_dir, 'd_icd_diagnoses.csv'), usecols=['icd_code', 'icd_version', 'long_title'])\n",
        "\n",
        "    # Identify sepsis ICD codes (A41*)\n",
        "    sepsis_codes = d_icd_diagnoses[\n",
        "        (d_icd_diagnoses['icd_version'] == 10) &\n",
        "        (d_icd_diagnoses['icd_code'].str.startswith('A41'))\n",
        "    ]['icd_code'].unique()\n",
        "\n",
        "    sepsis_stays = diagnoses_icd[\n",
        "        (diagnoses_icd['icd_version'] == 10) &\n",
        "        (diagnoses_icd['icd_code'].isin(sepsis_codes))\n",
        "    ][['subject_id', 'hadm_id']].drop_duplicates()\n",
        "\n",
        "    data['sepsis'] = data.apply(lambda row: 1 if ((row['subject_id'], row['hadm_id']) in sepsis_stays.set_index(['subject_id','hadm_id']).index) else 0, axis=1)\n",
        "    print(\"Sepsis distribution:\", data['sepsis'].value_counts(normalize=True))\n",
        "\n",
        "    # ---------------------------\n",
        "    # Step 2: Add Comorbidity Info (Number of distinct ICD diagnoses)\n",
        "    # ---------------------------\n",
        "    diagnosis_counts = diagnoses_icd.groupby(['subject_id', 'hadm_id'])['icd_code'].nunique().reset_index()\n",
        "    diagnosis_counts.rename(columns={'icd_code': 'num_diagnoses'}, inplace=True)\n",
        "    data = data.merge(diagnosis_counts, on=['subject_id', 'hadm_id'], how='left')\n",
        "    data['num_diagnoses'] = data['num_diagnoses'].fillna(0)\n",
        "\n",
        "    # ---------------------------\n",
        "    # Step 3: Extract Vital Signs (First 24h)\n",
        "    # ---------------------------\n",
        "    icustays_subset = icustays[['subject_id', 'hadm_id', 'stay_id', 'intime']]\n",
        "    icustays_subset['intime'] = pd.to_datetime(icustays_subset['intime'])\n",
        "\n",
        "    vital_signs = {\n",
        "        'Heart Rate': [211, 220045],\n",
        "        'SysBP': [51, 220179],\n",
        "        'DiasBP': [8368, 220180],\n",
        "        'MeanBP': [52, 220181],\n",
        "        'Respiratory Rate': [618, 220210],\n",
        "        'Temperature': [676, 223761],\n",
        "        'SpO2': [646, 220277]\n",
        "    }\n",
        "\n",
        "    print(\"Processing vital signs...\")\n",
        "    chartevents = dd.read_csv(os.path.join(data_dir, 'chartevents.csv'),\n",
        "                              usecols=['subject_id', 'hadm_id', 'stay_id', 'itemid', 'charttime', 'valuenum'],\n",
        "                              assume_missing=True, dtype={'valueuom': 'object'})\n",
        "\n",
        "    vital_itemids = [item for sublist in vital_signs.values() for item in sublist]\n",
        "    chartevents_filtered = chartevents[chartevents['itemid'].isin(vital_itemids)]\n",
        "    chartevents_filtered['charttime'] = dd.to_datetime(chartevents_filtered['charttime'])\n",
        "    chartevents_filtered = chartevents_filtered.merge(icustays_subset, on=['subject_id','hadm_id','stay_id'], how='inner')\n",
        "    chartevents_filtered['hours_in'] = (chartevents_filtered['charttime'] - chartevents_filtered['intime']).dt.total_seconds()/3600\n",
        "    chartevents_filtered = chartevents_filtered[(chartevents_filtered['hours_in'] >= 0) & (chartevents_filtered['hours_in'] <= 24)]\n",
        "\n",
        "    d_items = pd.read_csv(os.path.join(data_dir, 'd_items.csv'), usecols=['itemid','label'])\n",
        "    chartevents_filtered = chartevents_filtered.merge(d_items, on='itemid', how='left')\n",
        "\n",
        "    vitals = chartevents_filtered.groupby(['subject_id','hadm_id','stay_id','label'])['valuenum'].mean().compute().reset_index()\n",
        "    vitals_pivot = vitals.pivot_table(index=['subject_id','hadm_id','stay_id'], columns='label', values='valuenum').reset_index()\n",
        "\n",
        "    # ---------------------------\n",
        "    # Step 4: Extract Labs (WBC, Creatinine in first 24h)\n",
        "    # ---------------------------\n",
        "    print(\"Processing lab tests...\")\n",
        "    lab_tests = {\n",
        "        'WBC': [51300],\n",
        "        'Creatinine': [50912]\n",
        "    }\n",
        "    labevents = dd.read_csv(os.path.join(data_dir, 'labevents.csv'),\n",
        "                            usecols=['subject_id', 'hadm_id', 'itemid', 'charttime', 'valuenum'],\n",
        "                            assume_missing=True)\n",
        "    lab_itemids = [item for sublist in lab_tests.values() for item in sublist]\n",
        "    labevents_filtered = labevents[labevents['itemid'].isin(lab_itemids)]\n",
        "    labevents_filtered['charttime'] = dd.to_datetime(labevents_filtered['charttime'])\n",
        "\n",
        "    labevents_filtered = labevents_filtered.merge(icustays_subset, on=['subject_id','hadm_id'], how='inner')\n",
        "    labevents_filtered['hours_in'] = (labevents_filtered['charttime'] - labevents_filtered['intime']).dt.total_seconds()/3600\n",
        "    labevents_filtered = labevents_filtered[(labevents_filtered['hours_in'] >= 0) & (labevents_filtered['hours_in'] <= 24)]\n",
        "\n",
        "    d_labitems = pd.read_csv(os.path.join(data_dir, 'd_labitems.csv'), usecols=['itemid','label'])\n",
        "    labevents_filtered = labevents_filtered.merge(d_labitems, on='itemid', how='left')\n",
        "\n",
        "    labs = labevents_filtered.groupby(['subject_id','hadm_id','stay_id','label'])['valuenum'].mean().compute().reset_index()\n",
        "    labs_pivot = labs.pivot_table(index=['subject_id','hadm_id','stay_id'], columns='label', values='valuenum').reset_index()\n",
        "\n",
        "    # Merge vitals and labs\n",
        "    data = data.merge(vitals_pivot, on=['subject_id','hadm_id','stay_id'], how='left')\n",
        "    data = data.merge(labs_pivot, on=['subject_id','hadm_id','stay_id'], how='left')\n",
        "\n",
        "    # ---------------------------\n",
        "    # Step 5: Preprocessing\n",
        "    # ---------------------------\n",
        "    categorical_features = ['gender', 'anchor_year_group', 'insurance', 'marital_status']\n",
        "    data[categorical_features] = data[categorical_features].fillna('Unknown')\n",
        "    data = pd.get_dummies(data, columns=categorical_features, drop_first=True)\n",
        "\n",
        "    data = data.rename(columns={'anchor_age': 'age'})\n",
        "\n",
        "    exclude_cols = ['subject_id','hadm_id','stay_id','admittime','dischtime','intime','outtime','hospital_expire_flag','sepsis']\n",
        "    numeric_cols = [col for col in data.columns if col not in exclude_cols and data[col].dtype in [np.float64, np.int64]]\n",
        "\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    data[numeric_cols] = imputer.fit_transform(data[numeric_cols])\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
        "\n",
        "    target = 'sepsis'\n",
        "    data = data.dropna(subset=[target])\n",
        "    X = data.drop(columns=exclude_cols)\n",
        "    y = data[target].astype(int)\n",
        "\n",
        "    joblib.dump((X, y), processed_data_path)\n",
        "    print(f\"Processed data saved to '{processed_data_path}'.\")\n",
        "\n",
        "    X_np = np.ascontiguousarray(X.values)\n",
        "    y_np = np.ascontiguousarray(y.values)\n",
        "\n",
        "    # ---------------------------\n",
        "    # Step 6: Modeling\n",
        "    # ---------------------------\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_np, y_np, test_size=0.3, random_state=RANDOM_STATE, stratify=y_np)\n",
        "\n",
        "    models = {\n",
        "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=RANDOM_STATE),\n",
        "        'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
        "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
        "        'Naive Bayes': GaussianNB(),\n",
        "        'K-Nearest Neighbors': KNeighborsClassifier()\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    for model_name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred_proba = model.predict_proba(X_test)[:,1] if hasattr(model, \"predict_proba\") else y_pred\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        auc = roc_auc_score(y_test, y_pred_proba)\n",
        "        f1 = f1_score(y_test, y_pred)\n",
        "        results[model_name] = {'Accuracy': acc, 'AUC': auc, 'F1': f1}\n",
        "        print(f\"\\n{model_name}:\")\n",
        "        print(f\"Accuracy: {acc:.4f}, AUC: {auc:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "    # Add SVM with reduced sample\n",
        "    svm_model = SVC(probability=True, random_state=RANDOM_STATE, class_weight='balanced')\n",
        "    svm_fraction = 0.3\n",
        "    sample_size = int(len(X_train)*svm_fraction)\n",
        "    np.random.seed(RANDOM_STATE)\n",
        "    indices = np.random.choice(len(X_train), size=sample_size, replace=False)\n",
        "    X_train_svm = X_train[indices]\n",
        "    y_train_svm = y_train[indices]\n",
        "\n",
        "    svm_model.fit(X_train_svm, y_train_svm)\n",
        "    y_pred = svm_model.predict(X_test)\n",
        "    y_pred_proba = svm_model.predict_proba(X_test)[:,1]\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    results['Support Vector Machine'] = {'Accuracy': acc, 'AUC': auc, 'F1': f1}\n",
        "\n",
        "    print(\"\\nSupport Vector Machine (SVM) with reduced training data:\")\n",
        "    print(f\"Accuracy: {acc:.4f}, AUC: {auc:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "    print(\"\\nAll models evaluated successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "########################################################################\n",
        "# 1. Load Data\n",
        "########################################################################\n",
        "if __name__ == '__main__':\n",
        "    print(\"Loading previously saved processed data...\")\n",
        "    X, y = joblib.load('processed_data.pkl')\n",
        "    print(\"Data loaded. Shape of X:\", X.shape, \"Shape of y:\", y.shape)\n",
        "\n",
        "    if not isinstance(X, np.ndarray):\n",
        "        X = X.values\n",
        "    if not isinstance(y, np.ndarray):\n",
        "        y = y.values\n",
        "\n",
        "    ########################################################################\n",
        "    # 2. Define Models and Noise Levels\n",
        "    ########################################################################\n",
        "    models = {\n",
        "        'LR': LogisticRegression(max_iter=1000, random_state=RANDOM_STATE),\n",
        "        'DT': DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
        "        'RF': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
        "        'NB': GaussianNB(),\n",
        "        'KNN': KNeighborsClassifier(),\n",
        "        'SVM': SVC(probability=True, random_state=RANDOM_STATE, class_weight='balanced')\n",
        "    }\n",
        "\n",
        "    # Noise levels in ascending order: 0%, 5%, 10%, 15%, 20%, 25%, 30%, 35%, 40%\n",
        "    noise_levels = np.arange(0.0, 0.45, 0.05)\n",
        "\n",
        "    acc_results = {m: [] for m in models.keys()}\n",
        "    tpr_results = {m: [] for m in models.keys()}\n",
        "    tnr_results = {m: [] for m in models.keys()}\n",
        "\n",
        "    ########################################################################\n",
        "    # 3. Training, Evaluation, Confusion Matrices\n",
        "    ########################################################################\n",
        "    print(\"\\nEvaluating all models for each noise level over 10 runs...\\n\")\n",
        "\n",
        "    for model_key, model in models.items():\n",
        "        print(f\"--- {model_key} ---\")\n",
        "        for noise_level in noise_levels:\n",
        "            sum_tn, sum_fp, sum_fn, sum_tp = 0, 0, 0, 0\n",
        "\n",
        "            for iteration in range(10):\n",
        "                random_seed = RANDOM_STATE + iteration\n",
        "                X_train, X_test, y_train, y_test = train_test_split(\n",
        "                    X, y, test_size=0.5, stratify=y, random_state=random_seed\n",
        "                )\n",
        "\n",
        "                y_train_noisy = y_train.copy()\n",
        "                num_noisy = int(noise_level * len(y_train_noisy))\n",
        "                np.random.seed(random_seed)\n",
        "                noisy_indices = np.random.choice(len(y_train_noisy), size=num_noisy, replace=False)\n",
        "                # Flip labels\n",
        "                y_train_noisy[noisy_indices] = 1 - y_train_noisy[noisy_indices]\n",
        "\n",
        "                rus = RandomUnderSampler(random_state=random_seed)\n",
        "                X_train_bal, y_train_bal = rus.fit_resample(X_train, y_train_noisy)\n",
        "\n",
        "                model.fit(X_train_bal, y_train_bal)\n",
        "\n",
        "                y_pred = model.predict(X_test)\n",
        "\n",
        "                tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=[0, 1]).ravel()\n",
        "                sum_tn += tn\n",
        "                sum_fp += fp\n",
        "                sum_fn += fn\n",
        "                sum_tp += tp\n",
        "\n",
        "\n",
        "            total_runs = 10.0\n",
        "            # Summed over 10 runs\n",
        "            TN = sum_tn\n",
        "            FP = sum_fp\n",
        "            FN = sum_fn\n",
        "            TP = sum_tp\n",
        "            all_ = (TN + FP + FN + TP)\n",
        "\n",
        "            TPR = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
        "            TNR = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
        "            ACC = (TP + TN) / all_ if all_ > 0 else 0.0\n",
        "\n",
        "            acc_results[model_key].append(ACC * 100.0)\n",
        "            tpr_results[model_key].append(TPR * 100.0)\n",
        "            tnr_results[model_key].append(TNR * 100.0)\n",
        "\n",
        "            print(f\"Noise {int(noise_level * 100)}% -> Summed Confusion Matrix over 10 runs:\")\n",
        "            print(f\"   [[TN={TN}, FP={FP}],\")\n",
        "            print(f\"    [FN={FN}, TP={TP}]]\")\n",
        "            print(f\"   Accuracy={ACC:.3f}, TPR={TPR:.3f}, TNR={TNR:.3f}\")\n",
        "        print()\n",
        "\n",
        "    x_labels = np.array([100, 95, 90, 85, 80, 75, 70, 65, 60])\n",
        "    n_groups = len(x_labels)\n",
        "\n",
        "\n",
        "    for m in models.keys():\n",
        "        acc_results[m].reverse()\n",
        "        tpr_results[m].reverse()\n",
        "        tnr_results[m].reverse()\n",
        "\n",
        "    model_colors = {\n",
        "        'LR': '#1f77b4',  # blue\n",
        "        'DT': '#ff7f0e',  # orange\n",
        "        'RF': '#2ca02c',  # green\n",
        "        'NB': '#d62728',  # red\n",
        "        'KNN': '#9467bd',  # purple\n",
        "        'SVM': '#8c564b'  # brown\n",
        "    }\n",
        "\n",
        "\n",
        "    def plot_metric_sorted(dataset_name, metric_name, metric_data):\n",
        "        \"\"\"\n",
        "        Plots a grouped bar chart for a given dataset and metric.\n",
        "        Models are sorted in descending order based on their baseline (index 0) performance\n",
        "        in the reversed arrays (which corresponds to 0% noise => 100% correct labels).\n",
        "\n",
        "        X-axis: % accurate labels (descending order: 100, 95, …, 60)\n",
        "        Y-axis: Metric performance (0–100%)\n",
        "        \"\"\"\n",
        "        sorted_models = sorted(metric_data.keys(), key=lambda m: metric_data[m][0], reverse=True)\n",
        "        n_models = len(sorted_models)\n",
        "\n",
        "        x = np.arange(n_groups)\n",
        "        bar_width = 0.12\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        for i, model in enumerate(sorted_models):\n",
        "            plt.bar(x + i * bar_width, metric_data[model],\n",
        "                    width=bar_width,\n",
        "                    color=model_colors.get(model, 'grey'),\n",
        "                    label=model)\n",
        "\n",
        "        plt.xlabel('% Accurate Labels (Descending)')\n",
        "        plt.ylabel(f'{metric_name} (%)')\n",
        "        plt.title(f'{dataset_name} - {metric_name} vs. % Accurate Labels\\n(Models sorted by baseline performance)')\n",
        "        plt.xticks(x + (n_models / 2 - 0.5) * bar_width, x_labels)\n",
        "        plt.ylim(0, 100)\n",
        "        plt.legend(title='Model')\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    dataset_name = 'Sepsis Prediction (MIMIC-IV)'\n",
        "\n",
        "    plot_metric_sorted(dataset_name, 'Accuracy', acc_results)\n",
        "\n",
        "    plot_metric_sorted(dataset_name, 'TPR (Sensitivity)', tpr_results)\n",
        "\n",
        "\n",
        "    plot_metric_sorted(dataset_name, 'TNR (Specificity)', tnr_results)\n",
        "\n",
        "    print(\"All done!\")"
      ],
      "metadata": {
        "id": "4Sjvq7MFAQtk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}