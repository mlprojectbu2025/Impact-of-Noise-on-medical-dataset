{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Friedman Test Example\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import friedmanchisquare\n",
        "\n",
        "##############################################################################\n",
        "# 1) LOAD AND CLEAN THE DATA\n",
        "##############################################################################\n",
        "\n",
        "file_path = \"FF_Modelmetrics.csv\"\n",
        "df_raw = pd.read_csv(file_path)\n",
        "\n",
        "print(\"Initial DataFrame shape:\", df_raw.shape)\n",
        "print(df_raw.head())\n",
        "\n",
        "def parse_metric(value):\n",
        "\n",
        "    if isinstance(value, str):\n",
        "        if value.strip().endswith('%'):\n",
        "            try:\n",
        "                return float(value.strip().replace('%',''))\n",
        "            except:\n",
        "                return np.nan\n",
        "        else:\n",
        "            try:\n",
        "                return float(value)\n",
        "            except:\n",
        "                return np.nan\n",
        "    else:\n",
        "        return value\n",
        "\n",
        "metric_cols = ['Accuracy','TPR','TNR','F1 Score']\n",
        "for col in metric_cols:\n",
        "    df_raw[col] = df_raw[col].apply(parse_metric)\n",
        "\n",
        "\n",
        "def parse_noise(noise_str):\n",
        "    \"\"\"\n",
        "    Convert strings like \"0%\", \"5%\" to floats 0.0, 5.0, etc.\n",
        "    If it's already numeric, just return it.\n",
        "    \"\"\"\n",
        "    if isinstance(noise_str, str):\n",
        "        # strip whitespace\n",
        "        noise_str = noise_str.strip()\n",
        "        if noise_str.endswith('%'):\n",
        "            noise_val = noise_str.replace('%','')\n",
        "            try:\n",
        "                return float(noise_val)\n",
        "            except:\n",
        "                return np.nan\n",
        "        else:\n",
        "            # direct float if possible\n",
        "            try:\n",
        "                return float(noise_str)\n",
        "            except:\n",
        "                return np.nan\n",
        "    else:\n",
        "        return noise_str\n",
        "\n",
        "df_raw['Noise'] = df_raw['Noise'].apply(parse_noise)\n",
        "\n",
        "# Create a clean working copy\n",
        "df = df_raw.copy()\n",
        "\n",
        "print(\"\\nData after cleaning/conversion:\")\n",
        "print(df.head(10))\n",
        "print(df.info())\n",
        "\n",
        "\n",
        "\n",
        "model_list = sorted(df['Model'].unique())\n",
        "noise_list = sorted(df['Noise'].dropna().unique())\n",
        "print(\"\\nUnique Models:\", model_list)\n",
        "print(\"Unique Noise Levels:\", noise_list)\n",
        "\n",
        "from scipy.stats import friedmanchisquare\n",
        "\n",
        "friedman_results = []\n",
        "for model in model_list:\n",
        "    sub = df[df['Model'] == model].copy()\n",
        "\n",
        "    pivoted = sub.pivot_table(index='Dataset',\n",
        "                              columns='Noise',\n",
        "                              values='Accuracy')\n",
        "\n",
        "    pivoted = pivoted.dropna(axis=0, how='any')\n",
        "\n",
        "    if pivoted.shape[0] < 2 or pivoted.shape[1] < 2:\n",
        "\n",
        "        continue\n",
        "\n",
        "    pivoted = pivoted[sorted(pivoted.columns)]\n",
        "\n",
        "    data_for_test = [pivoted[col].values for col in pivoted.columns]\n",
        "\n",
        "    stat, pval = friedmanchisquare(*data_for_test)\n",
        "    friedman_results.append((model, stat, pval))\n",
        "\n",
        "friedman_df = pd.DataFrame(friedman_results,\n",
        "                           columns=['Model','Friedman_Stat','p_value'])\n",
        "\n",
        "print(\"\\nFriedman Test (by model across noise levels, aggregated by dataset blocks):\")\n",
        "print(friedman_df)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pEEDFijfY8a",
        "outputId": "9dba23e3-7e18-4087-8d6f-27331f3784a5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial DataFrame shape: (270, 12)\n",
            "           Dataset                     Model Noise      TN      FP     FN  \\\n",
            "0  sepsis balanced  Logistic Regression (LR)    0%  293999  121541  15138   \n",
            "1  sepsis balanced  Logistic Regression (LR)    5%  276228  139312  12661   \n",
            "2  sepsis balanced  Logistic Regression (LR)   10%  264774  150766  11382   \n",
            "3  sepsis balanced  Logistic Regression (LR)   15%  255915  159625  10692   \n",
            "4  sepsis balanced  Logistic Regression (LR)   20%  250847  164693  10376   \n",
            "\n",
            "      TP Accuracy    TPR    TNR    PPV  F1 Score  \n",
            "0  41612    0.711  0.733  0.708  0.255     0.378  \n",
            "1  44089    0.678  0.777  0.665  0.240     0.367  \n",
            "2  45368    0.657  0.799  0.637  0.231     0.359  \n",
            "3  46058    0.639  0.812  0.616  0.224     0.351  \n",
            "4  46374    0.629  0.817  0.604  0.220     0.346  \n",
            "\n",
            "Data after cleaning/conversion:\n",
            "           Dataset                     Model  Noise      TN      FP     FN  \\\n",
            "0  sepsis balanced  Logistic Regression (LR)    0.0  293999  121541  15138   \n",
            "1  sepsis balanced  Logistic Regression (LR)    5.0  276228  139312  12661   \n",
            "2  sepsis balanced  Logistic Regression (LR)   10.0  264774  150766  11382   \n",
            "3  sepsis balanced  Logistic Regression (LR)   15.0  255915  159625  10692   \n",
            "4  sepsis balanced  Logistic Regression (LR)   20.0  250847  164693  10376   \n",
            "5  sepsis balanced  Logistic Regression (LR)   25.0  246530  169010  10166   \n",
            "6  sepsis balanced  Logistic Regression (LR)   30.0  243598  171942  10504   \n",
            "7  sepsis balanced  Logistic Regression (LR)   35.0  239724  175816  11108   \n",
            "8  sepsis balanced  Logistic Regression (LR)   40.0  233946  181594  12245   \n",
            "9  sepsis balanced        Decision Tree (DT)    0.0  277377  138163  19327   \n",
            "\n",
            "      TP  Accuracy    TPR    TNR    PPV  F1 Score  \n",
            "0  41612     0.711  0.733  0.708  0.255     0.378  \n",
            "1  44089     0.678  0.777  0.665  0.240     0.367  \n",
            "2  45368     0.657  0.799  0.637  0.231     0.359  \n",
            "3  46058     0.639  0.812  0.616  0.224     0.351  \n",
            "4  46374     0.629  0.817  0.604  0.220     0.346  \n",
            "5  46584     0.621  0.821  0.593  0.216     0.343  \n",
            "6  46246     0.614  0.815  0.586  0.212     0.336  \n",
            "7  45642     0.604  0.804  0.577  0.206     0.328  \n",
            "8  44505     0.590  0.784  0.563  0.197     0.315  \n",
            "9  37423     0.667  0.659  0.668  0.213     0.322  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 270 entries, 0 to 269\n",
            "Data columns (total 12 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   Dataset   270 non-null    object \n",
            " 1   Model     270 non-null    object \n",
            " 2   Noise     270 non-null    float64\n",
            " 3   TN        270 non-null    int64  \n",
            " 4   FP        270 non-null    int64  \n",
            " 5   FN        270 non-null    int64  \n",
            " 6   TP        270 non-null    int64  \n",
            " 7   Accuracy  270 non-null    float64\n",
            " 8   TPR       270 non-null    float64\n",
            " 9   TNR       270 non-null    float64\n",
            " 10  PPV       270 non-null    float64\n",
            " 11  F1 Score  270 non-null    float64\n",
            "dtypes: float64(6), int64(4), object(2)\n",
            "memory usage: 25.4+ KB\n",
            "None\n",
            "\n",
            "Unique Models: ['Decision Tree (DT)', 'K-Nearest Neighbors (KNN)', 'Logistic Regression (LR)', 'Naive Bayes (NB)', 'Random Forest (RF)', 'Support Vector Machine (SVM)']\n",
            "Unique Noise Levels: [np.float64(0.0), np.float64(5.0), np.float64(10.0), np.float64(15.0), np.float64(20.0), np.float64(25.0), np.float64(30.0), np.float64(35.0), np.float64(40.0)]\n",
            "\n",
            "Friedman Test (by model across noise levels, aggregated by dataset blocks):\n",
            "                          Model  Friedman_Stat   p_value\n",
            "0            Decision Tree (DT)      39.305509  0.000004\n",
            "1     K-Nearest Neighbors (KNN)      39.786667  0.000004\n",
            "2      Logistic Regression (LR)      39.412354  0.000004\n",
            "3              Naive Bayes (NB)      15.610084  0.048313\n",
            "4            Random Forest (RF)      39.680000  0.000004\n",
            "5  Support Vector Machine (SVM)      38.346667  0.000006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################################################\n",
        "# 3) CORRELATION CHECK: ACCURACY vs. NOISE\n",
        "#    For each Model, we see how strongly performance declines with noise\n",
        "##############################################################################\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "print(\"\\n=== 3) CORRELATION CHECK: Accuracy vs. Noise (averaged across datasets) ===\")\n",
        "model_list = sorted(df['Model'].unique())\n",
        "for model in model_list:\n",
        "    sub = df[df['Model'] == model]\n",
        "    mean_perf = sub.groupby('Noise')['Accuracy'].mean().dropna().reset_index()\n",
        "\n",
        "    if mean_perf.shape[0] < 2:\n",
        "        continue\n",
        "\n",
        "    r, pval = pearsonr(mean_perf['Noise'], mean_perf['Accuracy'])\n",
        "    print(f\"Model={model:<40s} Pearson r={r:.3f}, p={pval:.3e}\")\n",
        "\n",
        "##############################################################################\n",
        "# 4) IDENTIFY COMMON FALL-OFF POINTS:\n",
        "#    Largest single-step drop from one noise level to the next\n",
        "##############################################################################\n",
        "\n",
        "print(\"\\n=== 4) COMMON FALL-OFF POINTS (largest single-step drop) ===\")\n",
        "\n",
        "falloff_records = []\n",
        "group_cols = ['Dataset','Model']\n",
        "\n",
        "for grp, sub_df in df.groupby(group_cols):\n",
        "    sub_df = sub_df.sort_values(by='Noise')\n",
        "    if sub_df['Accuracy'].isnull().all():\n",
        "        continue\n",
        "\n",
        "    sub_df['Acc_diff'] = sub_df['Accuracy'].diff()\n",
        "\n",
        "    min_diff = sub_df['Acc_diff'].min()\n",
        "    if pd.isnull(min_diff):\n",
        "        continue\n",
        "\n",
        "    idx_min_diff = sub_df['Acc_diff'].idxmin()\n",
        "    if idx_min_diff not in sub_df.index:\n",
        "        continue\n",
        "\n",
        "\n",
        "    row_ids = list(sub_df.index)\n",
        "    pos = row_ids.index(idx_min_diff)\n",
        "    if pos <= 0:\n",
        "        continue\n",
        "\n",
        "    from_idx = row_ids[pos - 1]\n",
        "    to_idx   = row_ids[pos]\n",
        "\n",
        "    from_row = sub_df.loc[from_idx]\n",
        "    to_row   = sub_df.loc[to_idx]\n",
        "\n",
        "    drop_val = to_row['Accuracy'] - from_row['Accuracy']\n",
        "    falloff_records.append({\n",
        "        'Dataset': grp[0],\n",
        "        'Model': grp[1],\n",
        "        'From_Noise': from_row['Noise'],\n",
        "        'To_Noise': to_row['Noise'],\n",
        "        'Drop_Value': drop_val\n",
        "    })\n",
        "\n",
        "falloff_df = pd.DataFrame(falloff_records)\n",
        "falloff_df.sort_values('Drop_Value', inplace=True)\n",
        "print(falloff_df.head(20))\n",
        "\n",
        "##############################################################################\n",
        "# 5) CROSSOVER ANALYSIS:\n",
        "#    Rank models at each noise level, see if rank order changes\n",
        "#    (We do this per dataset so that 'rank' is consistent within the dataset.)\n",
        "##############################################################################\n",
        "\n",
        "print(\"\\n=== 5) CROSSOVER ANALYSIS (ranking models) ===\")\n",
        "\n",
        "rank_list = []\n",
        "for dset, dsub in df.groupby('Dataset'):\n",
        "    for noise_val in sorted(dsub['Noise'].dropna().unique()):\n",
        "        noise_subset = dsub[dsub['Noise'] == noise_val]\n",
        "        noise_subset = noise_subset.copy()\n",
        "        noise_subset['Rank'] = noise_subset['Accuracy'].rank(method='average',\n",
        "                                                             ascending=False)\n",
        "        for _, row in noise_subset.iterrows():\n",
        "            rank_list.append({\n",
        "                'Dataset': dset,\n",
        "                'Noise': noise_val,\n",
        "                'Model': row['Model'],\n",
        "                'Accuracy': row['Accuracy'],\n",
        "                'Rank': row['Rank']\n",
        "            })\n",
        "\n",
        "rank_df = pd.DataFrame(rank_list)\n",
        "rank_df.sort_values(['Dataset','Noise','Rank'], inplace=True)\n",
        "print(\"Top 20 ranked results (lowest rank = best) by dataset & noise:\\n\")\n",
        "print(rank_df.head(20))\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "# 6) THRESHOLD DETECTION:\n",
        "#    Flag places where the slope is steeply negative or performance stabilizes\n",
        "##############################################################################\n",
        "\n",
        "print(\"\\n=== 6) THRESHOLD DETECTION (sharp decline vs. stabilization) ===\")\n",
        "\n",
        "threshold_records = []\n",
        "big_drop_cutoff = -0.05\n",
        "stabilize_cutoff = 0.01\n",
        "\n",
        "for grp, sub_df in df.groupby(['Dataset','Model']):\n",
        "    sub_df = sub_df.sort_values(by='Noise')\n",
        "    sub_df['Next_Acc'] = sub_df['Accuracy'].shift(-1)\n",
        "    sub_df['Delta'] = sub_df['Next_Acc'] - sub_df['Accuracy']\n",
        "\n",
        "    for i, row in sub_df.iterrows():\n",
        "        if pd.isnull(row['Delta']):\n",
        "            continue  # no next row\n",
        "        delta_val = row['Delta']\n",
        "\n",
        "        if delta_val <= big_drop_cutoff:\n",
        "            threshold_records.append({\n",
        "                'Dataset': grp[0],\n",
        "                'Model': grp[1],\n",
        "                'Noise': row['Noise'],\n",
        "                'Event': 'Significant Drop',\n",
        "                'Delta': delta_val\n",
        "            })\n",
        "        elif abs(delta_val) <= stabilize_cutoff:\n",
        "            threshold_records.append({\n",
        "                'Dataset': grp[0],\n",
        "                'Model': grp[1],\n",
        "                'Noise': row['Noise'],\n",
        "                'Event': 'Stabilization',\n",
        "                'Delta': delta_val\n",
        "            })\n",
        "\n",
        "threshold_df = pd.DataFrame(threshold_records)\n",
        "threshold_df.sort_values(['Dataset','Model','Noise'], inplace=True)\n",
        "print(threshold_df.head(30))\n",
        "\n",
        "print(\"\\n=== Done with correlation, fall-off, crossover, and threshold analyses. ===\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AL_E-wpgv5F",
        "outputId": "2e2ef56f-93ec-431d-d233-0480d8e1bd23"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== 3) CORRELATION CHECK: Accuracy vs. Noise (averaged across datasets) ===\n",
            "Model=Decision Tree (DT)                       Pearson r=-0.997, p=3.639e-09\n",
            "Model=K-Nearest Neighbors (KNN)                Pearson r=-0.935, p=2.132e-04\n",
            "Model=Logistic Regression (LR)                 Pearson r=-0.997, p=7.589e-09\n",
            "Model=Naive Bayes (NB)                         Pearson r=0.248, p=5.204e-01\n",
            "Model=Random Forest (RF)                       Pearson r=-0.968, p=1.963e-05\n",
            "Model=Support Vector Machine (SVM)             Pearson r=-0.989, p=5.085e-07\n",
            "\n",
            "=== 4) COMMON FALL-OFF POINTS (largest single-step drop) ===\n",
            "                  Dataset                         Model  From_Noise  To_Noise  \\\n",
            "25                 stroke     K-Nearest Neighbors (KNN)         0.0       5.0   \n",
            "27                 stroke              Naive Bayes (NB)        20.0      25.0   \n",
            "26                 stroke      Logistic Regression (LR)         0.0       5.0   \n",
            "28                 stroke            Random Forest (RF)        35.0      40.0   \n",
            "29                 stroke  Support Vector Machine (SVM)         0.0       5.0   \n",
            "14      genomics balanced      Logistic Regression (LR)        35.0      40.0   \n",
            "24                 stroke            Decision Tree (DT)        15.0      20.0   \n",
            "12      genomics balanced            Decision Tree (DT)        25.0      30.0   \n",
            "15      genomics balanced              Naive Bayes (NB)        35.0      40.0   \n",
            "16      genomics balanced            Random Forest (RF)        35.0      40.0   \n",
            "17      genomics balanced  Support Vector Machine (SVM)        30.0      35.0   \n",
            "13      genomics balanced     K-Nearest Neighbors (KNN)        20.0      25.0   \n",
            "21        sepsis balanced              Naive Bayes (NB)        35.0      40.0   \n",
            "18        sepsis balanced            Decision Tree (DT)         0.0       5.0   \n",
            "3   Heart Disease Dataset              Naive Bayes (NB)         5.0      10.0   \n",
            "10       diabetes dataset            Random Forest (RF)        35.0      40.0   \n",
            "19        sepsis balanced     K-Nearest Neighbors (KNN)         0.0       5.0   \n",
            "6        diabetes dataset            Decision Tree (DT)         5.0      10.0   \n",
            "0   Heart Disease Dataset            Decision Tree (DT)        10.0      15.0   \n",
            "5   Heart Disease Dataset  Support Vector Machine (SVM)        35.0      40.0   \n",
            "\n",
            "    Drop_Value  \n",
            "25     -11.100  \n",
            "27      -9.730  \n",
            "26      -6.580  \n",
            "28      -6.290  \n",
            "29      -5.180  \n",
            "14      -5.040  \n",
            "24      -4.620  \n",
            "12      -4.540  \n",
            "15      -4.480  \n",
            "16      -4.410  \n",
            "17      -4.060  \n",
            "13      -2.940  \n",
            "21      -0.288  \n",
            "18      -0.070  \n",
            "3       -0.068  \n",
            "10      -0.053  \n",
            "19      -0.048  \n",
            "6       -0.047  \n",
            "0       -0.047  \n",
            "5       -0.047  \n",
            "\n",
            "=== 5) CROSSOVER ANALYSIS (ranking models) ===\n",
            "Top 20 ranked results (lowest rank = best) by dataset & noise:\n",
            "\n",
            "                  Dataset  Noise                         Model  Accuracy  Rank\n",
            "1   Heart Disease Dataset    0.0      Logistic Regression (LR)     0.819   1.0\n",
            "0   Heart Disease Dataset    0.0  Support Vector Machine (SVM)     0.813   2.0\n",
            "2   Heart Disease Dataset    0.0     K-Nearest Neighbors (KNN)     0.795   3.0\n",
            "5   Heart Disease Dataset    0.0            Random Forest (RF)     0.789   4.0\n",
            "3   Heart Disease Dataset    0.0            Decision Tree (DT)     0.713   5.0\n",
            "4   Heart Disease Dataset    0.0              Naive Bayes (NB)     0.699   6.0\n",
            "6   Heart Disease Dataset    5.0  Support Vector Machine (SVM)     0.805   1.0\n",
            "7   Heart Disease Dataset    5.0      Logistic Regression (LR)     0.799   2.0\n",
            "11  Heart Disease Dataset    5.0            Random Forest (RF)     0.793   3.0\n",
            "8   Heart Disease Dataset    5.0     K-Nearest Neighbors (KNN)     0.788   4.0\n",
            "9   Heart Disease Dataset    5.0            Decision Tree (DT)     0.691   5.0\n",
            "10  Heart Disease Dataset    5.0              Naive Bayes (NB)     0.670   6.0\n",
            "12  Heart Disease Dataset   10.0  Support Vector Machine (SVM)     0.793   1.0\n",
            "13  Heart Disease Dataset   10.0      Logistic Regression (LR)     0.791   2.0\n",
            "14  Heart Disease Dataset   10.0     K-Nearest Neighbors (KNN)     0.785   3.5\n",
            "17  Heart Disease Dataset   10.0            Random Forest (RF)     0.785   3.5\n",
            "15  Heart Disease Dataset   10.0            Decision Tree (DT)     0.683   5.0\n",
            "16  Heart Disease Dataset   10.0              Naive Bayes (NB)     0.602   6.0\n",
            "18  Heart Disease Dataset   15.0  Support Vector Machine (SVM)     0.787   1.0\n",
            "23  Heart Disease Dataset   15.0            Random Forest (RF)     0.772   2.0\n",
            "\n",
            "=== 6) THRESHOLD DETECTION (sharp decline vs. stabilization) ===\n",
            "                  Dataset                         Model  Noise  \\\n",
            "0   Heart Disease Dataset            Decision Tree (DT)    5.0   \n",
            "1   Heart Disease Dataset            Decision Tree (DT)   15.0   \n",
            "2   Heart Disease Dataset     K-Nearest Neighbors (KNN)    0.0   \n",
            "3   Heart Disease Dataset     K-Nearest Neighbors (KNN)    5.0   \n",
            "4   Heart Disease Dataset      Logistic Regression (LR)    5.0   \n",
            "5   Heart Disease Dataset      Logistic Regression (LR)   15.0   \n",
            "6   Heart Disease Dataset              Naive Bayes (NB)    5.0   \n",
            "7   Heart Disease Dataset              Naive Bayes (NB)   15.0   \n",
            "8   Heart Disease Dataset              Naive Bayes (NB)   25.0   \n",
            "9   Heart Disease Dataset              Naive Bayes (NB)   30.0   \n",
            "10  Heart Disease Dataset            Random Forest (RF)    0.0   \n",
            "11  Heart Disease Dataset            Random Forest (RF)    5.0   \n",
            "12  Heart Disease Dataset  Support Vector Machine (SVM)    0.0   \n",
            "13  Heart Disease Dataset  Support Vector Machine (SVM)   10.0   \n",
            "14  Heart Disease Dataset  Support Vector Machine (SVM)   15.0   \n",
            "15       diabetes dataset            Decision Tree (DT)   10.0   \n",
            "16       diabetes dataset            Decision Tree (DT)   20.0   \n",
            "17       diabetes dataset     K-Nearest Neighbors (KNN)    0.0   \n",
            "18       diabetes dataset      Logistic Regression (LR)    0.0   \n",
            "19       diabetes dataset      Logistic Regression (LR)    5.0   \n",
            "20       diabetes dataset      Logistic Regression (LR)   10.0   \n",
            "21       diabetes dataset      Logistic Regression (LR)   15.0   \n",
            "22       diabetes dataset      Logistic Regression (LR)   20.0   \n",
            "23       diabetes dataset              Naive Bayes (NB)    0.0   \n",
            "24       diabetes dataset              Naive Bayes (NB)    5.0   \n",
            "25       diabetes dataset              Naive Bayes (NB)   10.0   \n",
            "26       diabetes dataset              Naive Bayes (NB)   15.0   \n",
            "27       diabetes dataset              Naive Bayes (NB)   20.0   \n",
            "28       diabetes dataset              Naive Bayes (NB)   25.0   \n",
            "29       diabetes dataset            Random Forest (RF)    0.0   \n",
            "\n",
            "               Event  Delta  \n",
            "0      Stabilization -0.008  \n",
            "1      Stabilization -0.006  \n",
            "2      Stabilization -0.007  \n",
            "3      Stabilization -0.003  \n",
            "4      Stabilization -0.008  \n",
            "5      Stabilization -0.003  \n",
            "6   Significant Drop -0.068  \n",
            "7      Stabilization  0.009  \n",
            "8      Stabilization -0.006  \n",
            "9      Stabilization  0.009  \n",
            "10     Stabilization  0.004  \n",
            "11     Stabilization -0.008  \n",
            "12     Stabilization -0.008  \n",
            "13     Stabilization -0.006  \n",
            "14     Stabilization -0.006  \n",
            "15     Stabilization  0.007  \n",
            "16     Stabilization  0.000  \n",
            "17     Stabilization -0.007  \n",
            "18     Stabilization -0.002  \n",
            "19     Stabilization -0.007  \n",
            "20     Stabilization  0.003  \n",
            "21     Stabilization -0.003  \n",
            "22     Stabilization -0.005  \n",
            "23     Stabilization  0.001  \n",
            "24     Stabilization -0.004  \n",
            "25     Stabilization -0.002  \n",
            "26     Stabilization  0.000  \n",
            "27     Stabilization -0.003  \n",
            "28     Stabilization  0.003  \n",
            "29     Stabilization -0.002  \n",
            "\n",
            "=== Done with correlation, fall-off, crossover, and threshold analyses. ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################################################\n",
        "# 4) IDENTIFY COMMON FALL-OFF POINTS (up to 25% noise ONLY)\n",
        "##############################################################################\n",
        "\n",
        "print(\"\\n=== 4) COMMON FALL-OFF POINTS (largest single-step drop, Noise ≤ 25) ===\")\n",
        "\n",
        "falloff_records = []\n",
        "group_cols = ['Dataset','Model']\n",
        "\n",
        "for grp, sub_df in df.groupby(group_cols):\n",
        "    # Restrict to Noise <= 25\n",
        "    sub_df = sub_df[sub_df['Noise'] <= 30].copy()\n",
        "\n",
        "    # Sort by Noise ascending\n",
        "    sub_df = sub_df.sort_values(by='Noise')\n",
        "    if sub_df['Accuracy'].isnull().all() or len(sub_df) < 2:\n",
        "        continue\n",
        "\n",
        "    # Compute consecutive Accuracy differences\n",
        "    sub_df['Acc_diff'] = sub_df['Accuracy'].diff()\n",
        "    min_diff = sub_df['Acc_diff'].min()\n",
        "    if pd.isnull(min_diff):\n",
        "        continue\n",
        "\n",
        "    idx_min_diff = sub_df['Acc_diff'].idxmin()\n",
        "    if idx_min_diff not in sub_df.index:\n",
        "        continue\n",
        "\n",
        "    row_ids = list(sub_df.index)\n",
        "    pos = row_ids.index(idx_min_diff)\n",
        "    if pos <= 0:\n",
        "        continue\n",
        "\n",
        "    from_idx = row_ids[pos - 1]\n",
        "    to_idx   = row_ids[pos]\n",
        "    from_row = sub_df.loc[from_idx]\n",
        "    to_row   = sub_df.loc[to_idx]\n",
        "\n",
        "    drop_val = to_row['Accuracy'] - from_row['Accuracy']\n",
        "    falloff_records.append({\n",
        "        'Dataset': grp[0],\n",
        "        'Model': grp[1],\n",
        "        'From_Noise': from_row['Noise'],\n",
        "        'To_Noise': to_row['Noise'],\n",
        "        'Drop_Value': drop_val\n",
        "    })\n",
        "\n",
        "falloff_df = pd.DataFrame(falloff_records)\n",
        "falloff_df.sort_values('Drop_Value', inplace=True)\n",
        "print(falloff_df.head(50))\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "# 6) THRESHOLD DETECTION (DROP-OFFS ONLY)\n",
        "##############################################################################\n",
        "\n",
        "print(\"\\n=== 6) THRESHOLD DETECTION (Significant Drop only) ===\")\n",
        "\n",
        "threshold_records = []\n",
        "big_drop_cutoff = -0.05\n",
        "\n",
        "for grp, sub_df in df.groupby(['Dataset','Model']):\n",
        "    sub_df = sub_df.sort_values(by='Noise')\n",
        "    sub_df['Next_Acc'] = sub_df['Accuracy'].shift(-1)\n",
        "    sub_df['Delta'] = sub_df['Next_Acc'] - sub_df['Accuracy']\n",
        "\n",
        "    for i, row in sub_df.iterrows():\n",
        "        if pd.isnull(row['Delta']):\n",
        "            continue\n",
        "        delta_val = row['Delta']\n",
        "\n",
        "        if delta_val <= big_drop_cutoff:\n",
        "            threshold_records.append({\n",
        "                'Dataset': grp[0],\n",
        "                'Model': grp[1],\n",
        "                'Noise': row['Noise'],\n",
        "                'Event': 'Significant Drop',\n",
        "                'Delta': delta_val\n",
        "            })\n",
        "\n",
        "threshold_df = pd.DataFrame(threshold_records)\n",
        "threshold_df.sort_values(['Dataset','Model','Noise'], inplace=True)\n",
        "print(threshold_df.head(30))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRVfyBjZklKT",
        "outputId": "e2462cb9-37bb-4689-d5b5-03c5836a7ded"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== 4) COMMON FALL-OFF POINTS (largest single-step drop, Noise ≤ 25) ===\n",
            "                  Dataset                         Model  From_Noise  To_Noise  \\\n",
            "25                 stroke     K-Nearest Neighbors (KNN)         0.0       5.0   \n",
            "27                 stroke              Naive Bayes (NB)        20.0      25.0   \n",
            "26                 stroke      Logistic Regression (LR)         0.0       5.0   \n",
            "29                 stroke  Support Vector Machine (SVM)         0.0       5.0   \n",
            "24                 stroke            Decision Tree (DT)        15.0      20.0   \n",
            "28                 stroke            Random Forest (RF)        20.0      25.0   \n",
            "17      genomics balanced  Support Vector Machine (SVM)        20.0      25.0   \n",
            "12      genomics balanced            Decision Tree (DT)        15.0      20.0   \n",
            "13      genomics balanced     K-Nearest Neighbors (KNN)        20.0      25.0   \n",
            "15      genomics balanced              Naive Bayes (NB)        20.0      25.0   \n",
            "16      genomics balanced            Random Forest (RF)        20.0      25.0   \n",
            "14      genomics balanced      Logistic Regression (LR)        20.0      25.0   \n",
            "21        sepsis balanced              Naive Bayes (NB)        10.0      15.0   \n",
            "18        sepsis balanced            Decision Tree (DT)         0.0       5.0   \n",
            "3   Heart Disease Dataset              Naive Bayes (NB)         5.0      10.0   \n",
            "19        sepsis balanced     K-Nearest Neighbors (KNN)         0.0       5.0   \n",
            "0   Heart Disease Dataset            Decision Tree (DT)        10.0      15.0   \n",
            "6        diabetes dataset            Decision Tree (DT)         5.0      10.0   \n",
            "1   Heart Disease Dataset     K-Nearest Neighbors (KNN)        20.0      25.0   \n",
            "4   Heart Disease Dataset            Random Forest (RF)        20.0      25.0   \n",
            "2   Heart Disease Dataset      Logistic Regression (LR)        20.0      25.0   \n",
            "20        sepsis balanced      Logistic Regression (LR)         0.0       5.0   \n",
            "22        sepsis balanced            Random Forest (RF)        20.0      25.0   \n",
            "7        diabetes dataset     K-Nearest Neighbors (KNN)        20.0      25.0   \n",
            "5   Heart Disease Dataset  Support Vector Machine (SVM)        20.0      25.0   \n",
            "10       diabetes dataset            Random Forest (RF)        20.0      25.0   \n",
            "23        sepsis balanced  Support Vector Machine (SVM)        20.0      25.0   \n",
            "11       diabetes dataset  Support Vector Machine (SVM)        10.0      15.0   \n",
            "8        diabetes dataset      Logistic Regression (LR)         5.0      10.0   \n",
            "9        diabetes dataset              Naive Bayes (NB)         5.0      10.0   \n",
            "\n",
            "    Drop_Value  \n",
            "25     -11.100  \n",
            "27      -9.730  \n",
            "26      -6.580  \n",
            "29      -5.180  \n",
            "24      -4.620  \n",
            "28      -4.220  \n",
            "17      -4.060  \n",
            "12      -3.850  \n",
            "13      -2.940  \n",
            "15      -2.930  \n",
            "16      -2.870  \n",
            "14      -2.240  \n",
            "21      -0.191  \n",
            "18      -0.070  \n",
            "3       -0.068  \n",
            "19      -0.048  \n",
            "0       -0.047  \n",
            "6       -0.047  \n",
            "1       -0.039  \n",
            "4       -0.037  \n",
            "2       -0.035  \n",
            "20      -0.033  \n",
            "22      -0.026  \n",
            "7       -0.022  \n",
            "5       -0.017  \n",
            "10      -0.016  \n",
            "23      -0.011  \n",
            "11      -0.010  \n",
            "8       -0.007  \n",
            "9       -0.004  \n",
            "\n",
            "=== 6) THRESHOLD DETECTION (Significant Drop only) ===\n",
            "                  Dataset                      Model  Noise             Event  \\\n",
            "0   Heart Disease Dataset           Naive Bayes (NB)    5.0  Significant Drop   \n",
            "1        diabetes dataset         Random Forest (RF)   35.0  Significant Drop   \n",
            "2       genomics balanced         Decision Tree (DT)    0.0  Significant Drop   \n",
            "3       genomics balanced         Decision Tree (DT)   10.0  Significant Drop   \n",
            "4       genomics balanced         Decision Tree (DT)   15.0  Significant Drop   \n",
            "5       genomics balanced         Decision Tree (DT)   25.0  Significant Drop   \n",
            "6       genomics balanced         Decision Tree (DT)   30.0  Significant Drop   \n",
            "7       genomics balanced         Decision Tree (DT)   35.0  Significant Drop   \n",
            "8       genomics balanced  K-Nearest Neighbors (KNN)    0.0  Significant Drop   \n",
            "9       genomics balanced  K-Nearest Neighbors (KNN)    5.0  Significant Drop   \n",
            "10      genomics balanced  K-Nearest Neighbors (KNN)   10.0  Significant Drop   \n",
            "11      genomics balanced  K-Nearest Neighbors (KNN)   15.0  Significant Drop   \n",
            "12      genomics balanced  K-Nearest Neighbors (KNN)   20.0  Significant Drop   \n",
            "13      genomics balanced  K-Nearest Neighbors (KNN)   25.0  Significant Drop   \n",
            "14      genomics balanced  K-Nearest Neighbors (KNN)   35.0  Significant Drop   \n",
            "15      genomics balanced   Logistic Regression (LR)    0.0  Significant Drop   \n",
            "16      genomics balanced   Logistic Regression (LR)    5.0  Significant Drop   \n",
            "17      genomics balanced   Logistic Regression (LR)   10.0  Significant Drop   \n",
            "18      genomics balanced   Logistic Regression (LR)   15.0  Significant Drop   \n",
            "19      genomics balanced   Logistic Regression (LR)   20.0  Significant Drop   \n",
            "20      genomics balanced   Logistic Regression (LR)   25.0  Significant Drop   \n",
            "21      genomics balanced   Logistic Regression (LR)   30.0  Significant Drop   \n",
            "22      genomics balanced   Logistic Regression (LR)   35.0  Significant Drop   \n",
            "23      genomics balanced           Naive Bayes (NB)    0.0  Significant Drop   \n",
            "24      genomics balanced           Naive Bayes (NB)    5.0  Significant Drop   \n",
            "25      genomics balanced           Naive Bayes (NB)   10.0  Significant Drop   \n",
            "26      genomics balanced           Naive Bayes (NB)   15.0  Significant Drop   \n",
            "27      genomics balanced           Naive Bayes (NB)   20.0  Significant Drop   \n",
            "28      genomics balanced           Naive Bayes (NB)   25.0  Significant Drop   \n",
            "29      genomics balanced           Naive Bayes (NB)   30.0  Significant Drop   \n",
            "\n",
            "    Delta  \n",
            "0  -0.068  \n",
            "1  -0.053  \n",
            "2  -3.080  \n",
            "3  -0.910  \n",
            "4  -3.850  \n",
            "5  -4.540  \n",
            "6  -1.680  \n",
            "7  -0.070  \n",
            "8  -0.760  \n",
            "9  -1.260  \n",
            "10 -2.870  \n",
            "11 -1.400  \n",
            "12 -2.940  \n",
            "13 -1.110  \n",
            "14 -0.830  \n",
            "15 -1.120  \n",
            "16 -1.110  \n",
            "17 -2.100  \n",
            "18 -2.100  \n",
            "19 -2.240  \n",
            "20 -2.660  \n",
            "21 -4.120  \n",
            "22 -5.040  \n",
            "23 -0.490  \n",
            "24 -0.490  \n",
            "25 -2.660  \n",
            "26 -1.190  \n",
            "27 -2.930  \n",
            "28 -1.610  \n",
            "29 -2.870  \n"
          ]
        }
      ]
    }
  ]
}